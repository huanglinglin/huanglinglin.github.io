{"meta":{"title":"怀古一何深","subtitle":"生活与coding呀","description":"机器学习","author":"Huaigu","url":"http://www.huaigu455study.com"},"pages":[{"title":"分类","date":"2019-02-25T07:22:26.000Z","updated":"2019-02-24T23:24:00.000Z","comments":true,"path":"categories/index.html","permalink":"http://www.huaigu455study.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-02-25T07:18:33.000Z","updated":"2019-02-24T23:20:58.000Z","comments":true,"path":"tags/index.html","permalink":"http://www.huaigu455study.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"神经网络 机器学习","slug":"人工神经网络","date":"2019-03-04T07:11:02.000Z","updated":"2019-03-04T00:07:18.000Z","comments":true,"path":"2019/03/04/人工神经网络/","link":"","permalink":"http://www.huaigu455study.com/2019/03/04/人工神经网络/","excerpt":"","text":"摘要 人工神经网络是一种通用的机器学习模型，是从生物学得到启发去模拟人脑的神经网络，期待能够实现人工智能的机器学习技术． 使用该算法的目的： 强大的线性分类器(监督学习) 不管是线性回归还是逻辑回归，我们都可以通过构造多项式来解决多项式回归问题，但是因为很多实例都需要非常复杂的多项式，当特征过多时，会导致过拟合，和计算量过大，速度缓慢等问题，我们以前学的Logistic算法不能很好的解决特征多的实例分类的问题，所以我们学会当下很火的神经网络算法是必须的！ 我认为logistic就可以看作一个最简单的神经网络模型(单层) 如果不对请指出． 单层神经网络的表示(神经元的表示) 不知道大家还记得摘要里说的不，神经网络是模拟人类大脑中的神经网络． 大家想想，我们高中生物里面学过的神经元，神经网络是由一个个神经元相互连接构成的． 如图： 轴突，树突． 神经元两端，有输入通道和输出通道． 这里不详细讲解了，大家自己去看老师视频哈哈．．．． 输入通道的数据经过神经元，然后从输出通道输出，好比神经末梢感受各种外部环境的变化，最后产生电信号的过程． 我们在这里使用一个很简单的神经元模型： 神经元模拟成一个逻辑单位 将x1,x2,x3 输入进圆圈中(这里的圆圈想象成神经元细胞体) ，而圆圈里就是我们要进行的操作． 提问 什么样的操作呢？ 看下文！ 这里我们还可以加一个x0. x0＝１，被称为偏置单元或偏置神经元，x0是我们看情况而定是否需要使用(看是不是更方便) 当我们这里的hthea(x) = 术语：我们可以说这是一个带有logistic/Sigmoid激活函数的人工神经网络 提问：为什么使用logistic/Sigmoid？原因是： 神经网络是一个强大的分类器 如果大家去看过老师的logistic算法的章节的话，从看它的数学公式，可以知道Sigmoid函数的值域范围限制在(0,1)之间,由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间，所以激活函数使用Sigmoid函数． thea被称为模型参数或者模型的权重！它控制着从某一层到后一层的映射 后续有图帮助理解 所以我们在神经元中进行的操作是什么呢？ 把我们上面的简单神经元模型具体化成运算过程 简述过程： 输入x1,x2,x3 所对应的权值(模型参数)分别为thea1,thea2,thea3 输出为z 这里的求和是 x1thea1+x2thea2+x3thea3 这里的非线性函数 是我们刚刚说到的Sigmoid函数 所以z = g(x1thea1+x2thea2+x3thea3) g函数就是Sigmoid函数 而神经网络是由许多个神经元连接在一起构成的 所以下面介绍双层神经网络 双层神经网络 与单层神经网络不同。两层神经网络可以无限逼近任意连续函数 这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层神经网络可以分类比单层更好！ 原因什么的 请自行google！！！ 两层神经网络有 输入层，隐藏层，输出层 如图： 第一个是输入层 第二个人是隐藏层 第三个是输出层 计算假设函数步骤如下 其实和单层是一样的 只不过多了一个隐藏层 容易理解 第一个代表激活项，例如a2^2代表第二层的第２个激活项激活项是指由一个具体的神经元计算并输出的值第二个代表权重矩阵它控制着从某一层到后一层的映射 如有不懂去看吴老师视频 p64课时 如图：计算步骤（结合上面的 神经元的操作的图去理解） 提示：所以这里的thea1 是一个３X4的矩阵 个人理解是三个激活项，４个权重值，（包含x0）依次可推．．．也可以用老师给的公式． 而这里的thea2则是一个1Ｘ3的矩阵，（除去x0参数） 如何高效的计算（简化计算）向量化实现 定义额外的项 看成z1^2 那么a1^2 = g(z1^2) 如图 通过观察发现每一个z是线性代数方程式，因此可以用矩阵乘法表示 输入的变量是[x0,x1，x2，x3]T（代表由x0,x1，x2，x3组成的列向量），用向量x来表示。方程的左边是[z1^2，z2^2,z3^2]T，用向量z^2来表示 如图： 所以可以得到 依次往下 这里我们把输入项x看成第一个a1 这个公式就是神经网络中从前一层计算后一层的矩阵运算 比如计算第二层则thea=thea1 ,a =a1 公式是 g(thea * a) = h_thea(x)也称为前向传播，从输入到输出 在神经网络中，不用输入的特征x1,x2,x3而是用激活项，也就是用逻辑回归训练的项作为输入进行训练！！！！！！！（每一层都是用上一层的来作为输入进行训练．） 每次一层选择好的thea可以得到更好的假设函数，比我们以前的logistic算法得到拟合的更好更复杂的函数！！！！！ 谢谢你看我的博客，如有写的不对的地方还请指出，文章只是用来记录我对于学习吴恩达视频的理解，也许还有很多细节没有提到，如果你通过我的博客有所收获那就是我的荣幸啦！后面还会继续更新有关于人工神经网络的博客会进行补充和深入 新手写博大佬误喷","categories":[{"name":"吴恩达机器学习","slug":"吴恩达机器学习","permalink":"http://www.huaigu455study.com/categories/吴恩达机器学习/"}],"tags":[{"name":"-机器学习 -学习笔记","slug":"机器学习-学习笔记","permalink":"http://www.huaigu455study.com/tags/机器学习-学习笔记/"}]},{"title":"线性回归 机器学习","slug":"机器学习线性回归","date":"2019-02-25T15:11:02.000Z","updated":"2019-03-03T23:11:00.000Z","comments":true,"path":"2019/02/25/机器学习线性回归/","link":"","permalink":"http://www.huaigu455study.com/2019/02/25/机器学习线性回归/","excerpt":"","text":"摘要：通过学习吴恩达课程，总结下自己对于线性回归模型的理解 线性回归 线性回归属于监督学习 线性回归的输出值是连续的 （此处提出是为了区别于分类的离散） 通过输入的特征值数目将线性回归分为单变量线性回归和多变量线性回归 也分别叫一元线性回归和多元线性回归 概念： 给定一个或多个自变量(特征)x和因变量y，拟合一条直线使得训练集的所有数据点与拟合直线之间的距离之和最小，可以采用平均平方距离来计算．从而对新的自变量所对应的因变量进行预测 一元线性回归 监督学习 简单来说就是我们通过已知的训练集样本构造得到的模型对未来数据进行预测 已知是指训练集中的单个样本都有正确答案给出 假设函数(预测的模型) 单变量线性回归的假设函数的公式 thea0,thea1 称为模型参数 随着thea0,thea1的变化，我们会得到不同的假设函数 所以此时我们的目标是找到能够与数据拟合的好的thea0,thea1…记着我们的目标 后续也是朝着这个目标行动 朝着这个目标的第一步是 我们如何判断线性回归拟合的直线好不好呢？ 这里我们需要引入代价函数 代价函数(cost function) 使用代价函数去对我们得到的假设函数进行评价！(与所有数据点是否拟合的好) 代价越小，则拟合的好 代价越大，则拟合的不好 所以我们要找到代价最小的点的thea1,thea0值！目标转为找到代价最小的点！！！ 优化目标转为：minmize J(thea1,thea0) 如何计算代价这里使用了平均平方距离之和 这里不理解的推荐去看吴老师的视频 链接放在底部 就很容易理解了 所以我们可以得到以下公式 公式中符号的含义: m = 训练集数量 表示向量x中的第i个元素 表示向量y中的第i个元素 不理解公式的请去反复看老师的视频这里就不推导过程了．．需要用图． 如果有不理解的地方．．评论插件弄好 大家可以讨论 为了更好的理解代价函数，我们简化参数将thea0 =0 可以得到thea1与代价函数为：这里的推导移步老师的视频 很容易理解 看到这张图！我们就应该开心啦 因为我们可以看到有一个代价最小的点 当thea1=1时代价为０ 是一个凸函数！！ ok 这时候回到刚刚的公式 两个模型参数thea1,thea0 此时代价函数和他们的关系： 碗状３维图形 代价值等于点到平面的距离 ok! 可以看出也有最低点 此时就完成了目标 用等高线来理解理解 高中地理应该学过吧．．． 不懂的话 多看几次视频！ 吴老师视频 还记得目标吗？ 是自动找到这个代价值最低的点 下个博客 引入我对于梯度下降算法的理解 谢谢观看我的博客如果我哪里有理解的不正确希望能通过评论告诉我评论功能完善ing…","categories":[{"name":"吴恩达机器学习","slug":"吴恩达机器学习","permalink":"http://www.huaigu455study.com/categories/吴恩达机器学习/"}],"tags":[{"name":"-机器学习 -学习笔记","slug":"机器学习-学习笔记","permalink":"http://www.huaigu455study.com/tags/机器学习-学习笔记/"}]},{"title":"梯度下降算法 机器学习","slug":"梯度下降算法","date":"2019-02-25T15:11:02.000Z","updated":"2019-03-03T23:11:26.000Z","comments":true,"path":"2019/02/25/梯度下降算法/","link":"","permalink":"http://www.huaigu455study.com/2019/02/25/梯度下降算法/","excerpt":"","text":"摘要 这篇博客我们介绍梯度下降算法(Gradient descent)前言 为什么需要使用梯度下降算法 移步线性回归 重要性 在机器学习扮演了非常重要的角色，在构造模型时，很多算法我们往往都会为了去解决一个最优化的问题，梯度下降则能用简单的原理去解决这个问题，所以学会梯度下降算法是非常重要的！ 维基百科 梯度下降法是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度的反方向的规定步长距离点进行迭代搜索。 看本文章之前介绍先去看老师的视频如果有不太懂得地方欢迎到这篇博客里面找一下，如果能让你理解，那便是我的荣幸啦 哈哈～～ 话不多说开始吧！ 理解梯度下降的思想(最优化) 这里的话我们需要想象我们在一个山上，我们如何快速下山的问题！ 移步视频理解这一段老师的讲解 吴恩达老师 p10.. 理解了我们就就可以观察图片得到算法的思路！！！ 算法思路 我们要做的就是开始给定thea0,thea1初始值 我们不停地一点点地改变thea0,thea1的值来使代价函数变小 直到我们找到Ｊ(thea)的最小值，或者局部最小值！ 一步一步慢慢下降到最低点 每一步都判断最优下降！ 这里我们先写出它的数学原理 我们在用可视化的图形去理解 可视化的图形我们会简化 h(thea) = thea0+thea1*x 将thea0=0 后续在两个参数一起理解(请记住同步更新模型参数!!)数学公式 这里我们可以看到有个α α这个参数我们需要好好理解一下 α在梯度下降算法中被称为学习率或者步长，使用α来控制步子的大小α越大，梯度下降迅速α越小，梯度下降缓慢 后果：α太小会导致下降速度慢！ α太大！则导致无法收敛到最低点或者发散！ 如图： 为了更好的理解梯度下降算法 我们简化假设函数并且可视化 可视化为： 那条虚线其实就是导数 所以结合数学公式 我们可以看出 thea1 = thea1 - α导数Ｊ(thea1) 所以thea1是在慢慢的变小 同理 如果导数在左侧则为负数 则thea1是在慢慢的变大的 结合上面： 理解多个模型参数是不是简单多了呢，请记住模型参数是同步更新 局部最优解 有些函数可能有多个局部极小值点，下面是一个例子 因为梯度下降算法适用于任意函数 在 A,B,C都有局部最小点，但是Ａ才是最优，不过梯度下降函数会在C,B就停止，这里停止是依据数学公式，在最低点的话导数为０，所以模型参数不会更新了． 后续还会将梯度算法运用在线性回归，Logistic算法上，记录一些学习到的新算法，持续关注！比心！谢谢你看我的博客，如果我有哪里理解的不对 还请指出感谢啦","categories":[{"name":"吴恩达机器学习","slug":"吴恩达机器学习","permalink":"http://www.huaigu455study.com/categories/吴恩达机器学习/"}],"tags":[{"name":"-机器学习 -学习笔记","slug":"机器学习-学习笔记","permalink":"http://www.huaigu455study.com/tags/机器学习-学习笔记/"}]},{"title":"制作属于自己的词云","slug":"python制作自己的词云","date":"2019-02-25T07:57:30.000Z","updated":"2019-02-26T00:46:16.000Z","comments":true,"path":"2019/02/25/python制作自己的词云/","link":"","permalink":"http://www.huaigu455study.com/2019/02/25/python制作自己的词云/","excerpt":"","text":"词云，又称文字云，是文本数据的视觉表示，由词汇组成类似云的图形(可以自定义图片)，用于展示大量文本数据。对网络文本中出现频率较高的“关键词”给以视觉上的突出(通过颜色或大小等)，帮助我们过滤掉大量的文本信息，使浏览者可以快速了解文本的主旨． 提问１为什么想要使用词云 1: 能够让我一眼了解文本主旨 快！ 2: 能够帮助我以后写论文和在ppt中加入 实用性高与别人对比起来会有加分项，更胜一筹． 提问２ 为什么能想到去实现词云 首先这里感谢下我曾经经常学习的地方[实验楼] (https://www.shiyanlou.com/)，是这里让我了解到词云 感谢这两个因素！ 好了我们一起进入正题吧！ 使用的词云的生成库类 wordcloud github地址 官方地址 安装 通过读官方库的说明 在github下 我们需要安装如下 12345 sudo pip3 install numpy sudo pip3 install Pillow sudo pip3 install matplotlib sudo pip3 install wordcloud sudo pip3 install jieba 安装成功后我们就可以开始弄词云啦 数据的准备 １．我们需要准备分析对象，文本 这里我们使用的文本是： ２．我们需要准备一张图片 文本预处理 在这里我们要对文本进行分词，因为在中文中，只有字，句和段落能够通过明显的分界符进行简单的划界，而对于＂词＂和＂词组＂来说，它们边界模糊，没有一个形式上的分解符． 由于这里我只看了一点人类简史就不导入自定义的词典了 用户自定义词典可以保证分词时，用户自定义的某些词能够被保留下来，例如“东野圭吾”这个作者，没被加入到自定义词典时，会被分成“东野”“圭吾”两个词，这肯定会影响后续研究，将其添加到词典中就不会被拆开了 这里我们使用jieba进行简单的分词!!….如果需要进行更精准的分词 进自行google jieba的用法 十几行代码实现： 皮卡丘词云： 这里提一句我第一次接触jieba 这里就简单分词一下，通过google发现还有许多分词方法(以后一定会在更新这篇文章)，同时我觉得还可以分成章节去分析章节分析每章内容 快速掌握章节知识点 好了结果就是这样啦！可以自定义图片形状，颜色．字体等等．．． 自行google学习，做一个属于自己的词云吧．． 感觉自己的词云还可以更加准确和个性化 这篇文章持续更新ing…..谢谢你看我的博客，如果我有哪里没写好或者你对于制作词云有更好的意见 /比如使用一些算法去分析文本等等 都可以在评论区告诉我．","categories":[{"name":"python有趣的小项目　","slug":"python有趣的小项目","permalink":"http://www.huaigu455study.com/categories/python有趣的小项目/"}],"tags":[{"name":"-随心更小项目 -python","slug":"随心更小项目-python","permalink":"http://www.huaigu455study.com/tags/随心更小项目-python/"}]},{"title":"欢迎来到我的博客","slug":"欢迎来到我的博客","date":"2019-02-25T07:57:30.000Z","updated":"2019-02-25T01:28:44.000Z","comments":true,"path":"2019/02/25/欢迎来到我的博客/","link":"","permalink":"http://www.huaigu455study.com/2019/02/25/欢迎来到我的博客/","excerpt":"","text":"欢迎来到我的博客这里分享我的日常和coding","categories":[{"name":"日常话唠　","slug":"日常话唠","permalink":"http://www.huaigu455study.com/categories/日常话唠/"}],"tags":[{"name":"-介绍","slug":"介绍","permalink":"http://www.huaigu455study.com/tags/介绍/"}]}]}